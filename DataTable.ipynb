{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TABLE CREATION",
   "id": "70191282af507847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from CreateTable import create_df\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data = create_df()\n",
    "# data.to_pickle(\"dataset.pkl\")\n",
    "\n",
    "# data = pd.read_pickle(\"dataset.pkl\")\n",
    "data.head()"
   ],
   "id": "8521e62c79249487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PLOTS",
   "id": "fcdfadeb7e07286e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def plot_aligned_samples(df, label_column, force_column, pos_column, threshold=0.1, max_samples_per_label=5):\n",
    "    unique_labels = (0, 1, 2, 3, 4, 5)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    used_labels = set()  # Track labels for legend\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Get all samples for this label\n",
    "        label_df = df[df[label_column] == label]\n",
    "\n",
    "        # Downsample if too many samples\n",
    "        if len(label_df) > max_samples_per_label:\n",
    "            label_df = resample(label_df, n_samples=max_samples_per_label, random_state=42)\n",
    "\n",
    "        for _, sample in label_df.iterrows():\n",
    "            # Extract force and position\n",
    "            force = np.array(sample[force_column])\n",
    "            position = np.array(sample[pos_column])\n",
    "\n",
    "            # Find alignment point\n",
    "            force_above_threshold = force > threshold\n",
    "            if np.any(force_above_threshold):\n",
    "                start_idx = np.where(force_above_threshold)[0][0]\n",
    "\n",
    "                # Label only the first occurrence of each label\n",
    "                label_text = f'Label {label}' if label not in used_labels else None\n",
    "                used_labels.add(label)\n",
    "\n",
    "                # Plot aligned data\n",
    "                axes.plot(position - position[start_idx], force, label=label_text, alpha=0.7)\n",
    "\n",
    "    # Set plot properties\n",
    "    axes.set_xlabel('Position')\n",
    "    axes.set_xlim(left=-0.001)\n",
    "    axes.set_ylabel('Force')\n",
    "    axes.set_title('Force over Time (Aligned)')\n",
    "    axes.legend()\n",
    "    axes.grid()\n",
    "\n",
    "    plt.suptitle('Aligned Sample Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_aligned_samples(data, label_column='label', force_column='Fi', pos_column='Pi', threshold=0.05,\n",
    "                     max_samples_per_label=2)\n"
   ],
   "id": "b43f1becae2eabd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_aligned_samples(df, label_column, force_column, pos_column, time_column, n=1, threshold=0.1):\n",
    "    unique_labels = df[label_column].unique()  # Get unique labels\n",
    "    unique_labels = (0, 1, 2, 3, 4, 5)\n",
    "\n",
    "    for i in range(n):  # Repeat n times\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "        for label in unique_labels:\n",
    "            # Randomly select one sample per label\n",
    "            sample = df[df[label_column] == label].sample(1, random_state=np.random.randint(0, 10000))\n",
    "\n",
    "            # Extract force, position, and time\n",
    "            force = np.array(sample[force_column].tolist()[0])\n",
    "            position = np.array(sample[pos_column].tolist()[0])\n",
    "            time = np.array(sample[time_column].tolist()[0])\n",
    "\n",
    "            # Find the first index where force rises above the threshold\n",
    "            start_idx = np.where(force > threshold)[0][0]\n",
    "\n",
    "            # Align the time axis so that this point is at t = 0\n",
    "            time_aligned = time - time[start_idx]\n",
    "\n",
    "            # Plot force vs. time (left subplot)\n",
    "            axes[0].plot(time_aligned, force, label=f'Label {label}')\n",
    "            axes[0].set_xlabel('Time (Aligned)')\n",
    "            axes[0].set_ylabel('Force')\n",
    "            axes[0].set_title('Force over Time (Aligned)')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid()\n",
    "\n",
    "            # Plot position vs. time (right subplot)\n",
    "            axes[1].plot(time_aligned, position, label=f'Label {label}')\n",
    "            axes[1].set_xlabel('Time (Aligned)')\n",
    "            axes[1].set_ylabel('Position')\n",
    "            axes[1].set_title('Position over Time (Aligned)')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid()\n",
    "\n",
    "        plt.suptitle(f'Aligned Sample Plot {i + 1}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_aligned_samples(data, label_column='label', force_column='Fi', pos_column='Pi', time_column='Timei', n=2,\n",
    "                     threshold=0.05)"
   ],
   "id": "5daa61e410e7acdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SPATIAL SMOOTHING",
   "id": "17ad70bb5a386364"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.interpolate import NearestNDInterpolator\n",
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper import *\n",
    "\n",
    "smoothing_config = {\n",
    "    'Entropy': 'gaussian',\n",
    "} # Dictionary 'feature-name' : type of smoothing (set if you want to test other method of smoothing instead of median)\n",
    "\n",
    "def apply_smoothing(grid_z, method='median'):\n",
    "    \"\"\"\n",
    "    Apply different smoothing techniques to the grid\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_z : numpy.ndarray\n",
    "        Input grid to be smoothed\n",
    "    method : str, optional (default='median')\n",
    "        Smoothing method to use. Options:\n",
    "        - 'median': Median filter\n",
    "        - 'gaussian': Gaussian filter\n",
    "        - 'diffusion': Anisotropic diffusion\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Smoothed grid\n",
    "    \"\"\"\n",
    "    if method == 'median':\n",
    "        return median_filter(grid_z, size = 3, mode = 'reflect')\n",
    "\n",
    "    elif method == 'gaussian':\n",
    "        return gaussian_filter(grid_z, sigma = 2)\n",
    "\n",
    "    elif method == 'diffusion':\n",
    "        # Simple anisotropic diffusion\n",
    "        def diffusion_step(img, kappa=50):\n",
    "            \"\"\"\n",
    "            Perform a single diffusion step\n",
    "            \"\"\"\n",
    "            # Compute image gradients\n",
    "            dy, dx = np.gradient(img)\n",
    "\n",
    "            # Compute diffusion coefficients\n",
    "            diff_coef_x = 1 / (1 + (dx/kappa)**2)\n",
    "            diff_coef_y = 1 / (1 + (dy/kappa)**2)\n",
    "\n",
    "            # Compute diffusion\n",
    "            diff_x = np.zeros_like(img)\n",
    "            diff_y = np.zeros_like(img)\n",
    "\n",
    "            diff_x[1:-1, 1:-1] = diff_coef_x[1:-1, 1:-1] * (img[1:-1, 2:] - img[1:-1, 1:-1])\n",
    "            diff_y[1:-1, 1:-1] = diff_coef_y[1:-1, 1:-1] * (img[2:, 1:-1] - img[1:-1, 1:-1])\n",
    "\n",
    "            return img + 0.25 * (diff_x + diff_y)\n",
    "\n",
    "        # Apply multiple diffusion steps\n",
    "        iterations = 20\n",
    "        img = grid_z.copy()\n",
    "        for i in range(iterations):\n",
    "            img = diffusion_step(img)\n",
    "        return img\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Smoothing method not implemented: {method}\")\n",
    "\n",
    "def get_grid_bounds(df):\n",
    "    x_min, x_max = int(df[\"posx\"].min()), int(df[\"posx\"].max())\n",
    "    y_min, y_max = int(df[\"posy\"].min()), int(df[\"posy\"].max())\n",
    "    grid_shape = (y_max - y_min + 1, x_max - x_min + 1)\n",
    "    return x_min, y_min, grid_shape\n",
    "\n",
    "# Smoothing function for a single dataframe\n",
    "def smooth_subset(subset_df, x_min, y_min, grid_shape):\n",
    "    smoothed_subset = pd.DataFrame(index=subset_df.index)\n",
    "\n",
    "    for feature in feature_list:\n",
    "        # Initialize grid with NaNs\n",
    "        grid_z = np.full(grid_shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Map each data point to the grid\n",
    "        for _, row in subset_df.iterrows():\n",
    "            x_idx = int(row[\"posx\"]) - x_min\n",
    "            y_idx = int(row[\"posy\"]) - y_min\n",
    "            grid_z[y_idx, x_idx] = row[feature]\n",
    "\n",
    "        # Interpolate missing values\n",
    "        yy, xx = np.indices(grid_z.shape)\n",
    "        valid_mask = ~np.isnan(grid_z)\n",
    "\n",
    "        if np.any(~valid_mask):\n",
    "            interpolator = NearestNDInterpolator(\n",
    "                np.column_stack((yy[valid_mask], xx[valid_mask])),\n",
    "                grid_z[valid_mask]\n",
    "            )\n",
    "            grid_z = interpolator(yy, xx)\n",
    "\n",
    "        # Apply feature-specific smoothing\n",
    "        methods = smoothing_config.get(feature, 'median') # Get the designed methods, if None select defaul 'median' method\n",
    "        if isinstance(methods, list):\n",
    "            for method in methods:\n",
    "                grid_z = apply_smoothing(grid_z, method=method)\n",
    "        else:\n",
    "            grid_z = apply_smoothing(grid_z, method=methods)\n",
    "\n",
    "        # Map smoothed grid back to DataFrame\n",
    "        smoothed_subset[feature] = [\n",
    "            grid_z[int(row[\"posy\"]) - y_min, int(row[\"posx\"]) - x_min]\n",
    "            for _, row in subset_df.iterrows()\n",
    "        ]\n",
    "\n",
    "    # Add back metadata columns\n",
    "    smoothed_subset[['label', 'posx', 'posy','Row']] = subset_df[['label', 'posx', 'posy','Row']]\n",
    "    return smoothed_subset\n",
    "\n",
    "if smoothing_config is None:\n",
    "    smoothing_config = {feature: 'median' for feature in feature_list}\n",
    "\n",
    "# Split data into train (posy < 90), train (posy > 110), and test\n",
    "test_df = data[data['Row'] == 'Test']\n",
    "train_df_lower = data[(data['Row'] == 'Train') & (data['posy'] < 110)]\n",
    "train_df_upper = data[(data['Row'] == 'Train') & (data['posy'] > 125)]\n",
    "\n",
    "# Compute grid bounds and smooth each subset\n",
    "test_x_min, test_y_min, test_grid_shape = get_grid_bounds(test_df)\n",
    "train_lower_x_min, train_lower_y_min, train_lower_grid_shape = get_grid_bounds(train_df_lower)\n",
    "train_upper_x_min, train_upper_y_min, train_upper_grid_shape = get_grid_bounds(train_df_upper)\n",
    "\n",
    "# Smooth each subset SEPARATELY\n",
    "smoothed_test = smooth_subset(test_df, test_x_min, test_y_min, test_grid_shape)\n",
    "smoothed_train_lower = smooth_subset(train_df_lower, train_lower_x_min, train_lower_y_min, train_lower_grid_shape)\n",
    "smoothed_train_upper = smooth_subset(train_df_upper, train_upper_x_min, train_upper_y_min, train_upper_grid_shape)\n",
    "\n",
    "smoothed_train = pd.concat([smoothed_train_lower, smoothed_train_upper], ignore_index=True)\n",
    "smoothed_df = pd.concat([smoothed_test, smoothed_train], ignore_index=True) # For visualization in the next sections only"
   ],
   "id": "99d148d49061a836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DB SPLITTING",
   "id": "4b301ca0f0f71e01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "# Display class distribution\n",
    "X = smoothed_df[feature_list + [\"posx\", \"posy\"]]\n",
    "y = smoothed_df['label']\n",
    "class_distribution = Counter(y)\n",
    "print(\"Class distribution in the dataset:\")\n",
    "for label, count in sorted(class_distribution.items()):\n",
    "    print(f\"Label {label}: {count} samples ({count / len(y) * 100:.2f}%)\")\n",
    "\n",
    "# Prepare data for classification\n",
    "X_train = smoothed_train[feature_list + [\"posx\", \"posy\"]]\n",
    "y_train = smoothed_train['label']\n",
    "X_test = smoothed_test[feature_list + [\"posx\", \"posy\"]]\n",
    "y_test = smoothed_test['label']\n",
    "\n",
    "# Preprocess data  -  No need a scaler in Random FOREST!!!!, if other model, apply after the splitting\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = X_train\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_list + [\"posx\", \"posy\"], index=X.index)\n",
    "X_test_scaled = X_test\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_list + [\"posx\", \"posy\"], index=X.index)\n",
    "\n",
    "# Create the figure and axes for the big plot and the smaller ones\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 8), gridspec_kw={'width_ratios': [3, 1, 1, 1]})\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Main plot for posx vs posy (Train vs Test)\n",
    "sns.scatterplot(data=smoothed_df, x='posx', y='posy', hue='Row', palette='Set2', ax=axs[0])\n",
    "axs[0].set_title(\"Train and Test Set Distribution (posx vs posy)\")\n",
    "axs[0].set_xlabel(\"Position X (posx)\")\n",
    "axs[0].set_ylabel(\"Position Y (posy)\")\n",
    "axs[0].legend(title='Set')\n",
    "\n",
    "plot_class_distribution(smoothed_df['label'], axs[1], \"Class Distribution (Entire Dataset)\")\n",
    "plot_class_distribution(y_train, axs[2], \"Class Distribution (Training Set)\")\n",
    "plot_class_distribution(y_test, axs[3], \"Class Distribution (Test Set)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Drop posx and posy columns from the training and testing sets after plotting\n",
    "# X_train = X_train.drop(columns=['posx', 'posy'])\n",
    "# X_test = X_test.drop(columns=['posx', 'posy'])"
   ],
   "id": "934212b578e97c7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Binary Classifier",
   "id": "25ce08865461ab20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# Convert labels to binary classification (0 vs all)\n",
    "y_train_binary = np.where(y_train == 0, 0, 1)\n",
    "y_test_binary = np.where(y_test == 0, 0, 1)\n",
    "\n",
    "# Create dataset with selected features (excluding posx, posy)\n",
    "X_train_selected = X_train[feature_list]\n",
    "X_test_selected = X_test[feature_list]\n",
    "\n",
    "# Apply ADASYN for class imbalance handling\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42, n_neighbors=5)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_selected, y_train_binary)\n",
    "\n",
    "# Display new class distribution after ADASYN\n",
    "adasyn_class_distribution = Counter(y_train_adasyn)\n",
    "print(\"\\nClass distribution after ADASYN:\")\n",
    "for label, count in sorted(adasyn_class_distribution.items()):\n",
    "    print(f\"Label {label}: {count} samples ({count / len(y_train_adasyn) * 100:.2f}%)\")\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    bootstrap=True,\n",
    "    criterion='entropy',\n",
    "    class_weight=None,\n",
    "    random_state=42,\n",
    "    max_features=0.5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model on binary data\n",
    "rf_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# Make predictions on training data\n",
    "y_train_pred = rf_model.predict(X_train_adasyn)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_test_pred = rf_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(\"\\nClassification Report (Test Data):\")\n",
    "print(classification_report(y_test_binary, y_test_pred, zero_division=0))\n",
    "\n",
    "# Calculate performance metrics\n",
    "test_accuracy = accuracy_score(y_test_binary, y_test_pred)\n",
    "test_balanced_acc = balanced_accuracy_score(y_test_binary, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train_adasyn, y_train_pred)\n",
    "train_balanced_acc = balanced_accuracy_score(y_train_adasyn, y_train_pred)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_accuracy:.3f}, Train Balanced Accuracy: {train_balanced_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}, Test Balanced Accuracy: {test_balanced_acc:.3f}\")\n",
    "\n",
    "# Function to create and plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_norm = np.nan_to_num(cm_norm)  # Replace NaN with 0\n",
    "\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1], ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "# Create a figure with two subplots for the confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot confusion matrices for training and test data\n",
    "plot_confusion_matrix(y_train_adasyn, y_train_pred, 'Training Data Confusion Matrix', ax1)\n",
    "plot_confusion_matrix(y_test_binary, y_test_pred, 'Test Data Confusion Matrix', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "376e50309c20825d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract posx and posy from X_test (ensure they are NumPy arrays)\n",
    "posx = np.array(X_test['posx'])\n",
    "posy = np.array(X_test['posy'])\n",
    "\n",
    "# Ensure y_test and y_test_pred are NumPy arrays\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "\n",
    "# Identify misclassified points\n",
    "misclassified = y_test_binary != y_test_pred\n",
    "\n",
    "# Create figure with two vertically stacked subplots\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "# Use 'plasma' colormap for better contrast\n",
    "cmap = 'turbo'\n",
    "\n",
    "# Initialize lists to store legend handles\n",
    "scatter_plots = []\n",
    "\n",
    "# First subplot: Color by true labels\n",
    "sc = axes[0].scatter(posx[~misclassified], posy[~misclassified],\n",
    "                      c=y_test_binary[~misclassified], cmap=cmap, marker='o', label='Correctly Classified')\n",
    "\n",
    "sc_misclassified = axes[0].scatter(posx[misclassified], posy[misclassified],\n",
    "                                   c=y_test_binary[misclassified], cmap=cmap, marker='x',\n",
    "                                   linewidth=1, label='Misclassified')\n",
    "\n",
    "axes[0].set_title('True Labels with Misclassified Points')\n",
    "axes[0].set_ylabel('posy')\n",
    "\n",
    "# Second subplot: Color by predicted labels\n",
    "sc_pred = axes[1].scatter(posx[~misclassified], posy[~misclassified],\n",
    "                           c=y_test_pred[~misclassified], cmap=cmap, marker='o', label='Correctly Classified')\n",
    "\n",
    "sc_pred_misclassified = axes[1].scatter(posx[misclassified], posy[misclassified],\n",
    "                                        c=y_test_pred[misclassified], cmap=cmap, marker='x',\n",
    "                                        linewidth=1, label='Misclassified')\n",
    "\n",
    "axes[1].set_title('Predicted Labels with Misclassified Points')\n",
    "axes[1].set_xlabel('posx')\n",
    "axes[1].set_ylabel('posy')\n",
    "\n",
    "# Collect legend handles and labels\n",
    "handles = [sc, sc_misclassified]\n",
    "labels = ['Correctly Classified', 'Misclassified']\n",
    "\n",
    "# Create one global legend outside the plot\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=2, fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c35ffc4e5615f171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Full Classifier",
   "id": "b52a137be2c5df8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create dataset with selected features (no posx, posy)\n",
    "X_train_selected = X_train[feature_list]\n",
    "X_test_selected = X_test[feature_list]\n",
    "\n",
    "# Apply ADASYN for adaptive oversampling\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42, n_neighbors=5)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Display new class distribution after ADASYN\n",
    "adasyn_class_distribution = Counter(y_train_adasyn)\n",
    "print(\"\\nClass distribution after ADASYN:\")\n",
    "for label, count in sorted(adasyn_class_distribution.items()):\n",
    "    print(f\"Label {label}: {count} samples ({count / len(y_train_adasyn) * 100:.2f}%)\")\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    bootstrap=True,\n",
    "    criterion='entropy',\n",
    "    class_weight=None,\n",
    "    random_state=42,\n",
    "    max_features=0.5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# Make predictions on training data\n",
    "y_train_pred = rf_model.predict(X_train_adasyn)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_test_pred = rf_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(\"\\nClassification Report (Test Data):\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "# Calculate performance metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "train_accuracy = accuracy_score(y_train_adasyn, y_train_pred)\n",
    "train_balanced_acc = balanced_accuracy_score(y_train_adasyn, y_train_pred)\n",
    "\n",
    "\n",
    "# Function to create and plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_norm = np.nan_to_num(cm_norm)  # Replace NaN with 0\n",
    "\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues', xticklabels=sorted(np.unique(y)),\n",
    "                yticklabels=sorted(np.unique(y)), ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "\n",
    "# Create a figure with two subplots for the confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot confusion matrices for training and test data\n",
    "plot_confusion_matrix(y_train_adasyn, y_train_pred, 'Training Data Confusion Matrix', ax1)\n",
    "plot_confusion_matrix(y_test, y_test_pred, 'Test Data Confusion Matrix', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4478eee696ff2186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract posx and posy from X_test (ensure they are NumPy arrays)\n",
    "posx = np.array(X_test['posx'])\n",
    "posy = np.array(X_test['posy'])\n",
    "\n",
    "# Ensure y_test and y_test_pred are NumPy arrays\n",
    "y_test = np.array(y_test)\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "\n",
    "# Identify misclassified points\n",
    "misclassified = y_test != y_test_pred\n",
    "\n",
    "# Create figure with two vertically stacked subplots\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "# Use 'plasma' colormap for better contrast\n",
    "cmap = 'turbo'\n",
    "\n",
    "# Initialize lists to store legend handles\n",
    "scatter_plots = []\n",
    "\n",
    "# First subplot: Color by true labels\n",
    "sc = axes[0].scatter(posx[~misclassified], posy[~misclassified],\n",
    "                      c=y_test[~misclassified], cmap=cmap, marker='o', label='Correctly Classified')\n",
    "\n",
    "sc_misclassified = axes[0].scatter(posx[misclassified], posy[misclassified],\n",
    "                                   c=y_test[misclassified], cmap=cmap, marker='x',\n",
    "                                   linewidth=1, label='Misclassified')\n",
    "\n",
    "axes[0].set_title('True Labels with Misclassified Points')\n",
    "axes[0].set_ylabel('posy')\n",
    "\n",
    "# Second subplot: Color by predicted labels\n",
    "sc_pred = axes[1].scatter(posx[~misclassified], posy[~misclassified],\n",
    "                           c=y_test_pred[~misclassified], cmap=cmap, marker='o', label='Correctly Classified')\n",
    "\n",
    "sc_pred_misclassified = axes[1].scatter(posx[misclassified], posy[misclassified],\n",
    "                                        c=y_test_pred[misclassified], cmap=cmap, marker='x',\n",
    "                                        linewidth=1, label='Misclassified')\n",
    "\n",
    "axes[1].set_title('Predicted Labels with Misclassified Points')\n",
    "axes[1].set_xlabel('posx')\n",
    "axes[1].set_ylabel('posy')\n",
    "\n",
    "# Collect legend handles and labels\n",
    "handles = [sc, sc_misclassified]\n",
    "labels = ['Correctly Classified', 'Misclassified']\n",
    "\n",
    "# Create one global legend outside the plot\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=2, fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e8ca12e18e353c6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# --- Random Forest Feature Importances ---\n",
    "\n",
    "# Assuming rf_model is your trained model and top_features is your list of features\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances and sort them\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_list,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "# --- Mutual Information Scores ---\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'label' is your target variable\n",
    "X = data[feature_list]\n",
    "y = data['label']\n",
    "\n",
    "# Compute mutual information scores for each feature\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "X2 = smoothed_df[feature_list]\n",
    "y2 = smoothed_df['label']\n",
    "\n",
    "# Compute mutual information scores for each feature (smoothed)\n",
    "mi_scores2 = mutual_info_classif(X2, y2, random_state=42)\n",
    "\n",
    "# Create a DataFrame for mutual information scores and sort them\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': feature_list,\n",
    "    'mutual_info': mi_scores,\n",
    "    'mutual_info2': mi_scores2\n",
    "}).sort_values('mutual_info', ascending=True)\n",
    "\n",
    "# Create subplots with 1 row and 2 columns and share the y-axis for alignment\n",
    "_fig, axes = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "\n",
    "# Plot Random Forest feature importances\n",
    "sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis', ax=axes[0], hue=\"feature\", legend=False)\n",
    "axes[0].set_title('Random Forest Feature Importances', fontsize=16)\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_ylabel('Features', fontsize=12)\n",
    "\n",
    "# Plot Mutual Information scores (Original vs Smoothed)\n",
    "ax = axes[1]\n",
    "sns.barplot(x='mutual_info', y='feature', data=mi_df, color='red', label='Original', alpha=0.6, ax=ax)\n",
    "sns.barplot(x='mutual_info2', y='feature', data=mi_df, color='blue', label='Smoothed', alpha=0.6, ax=ax)\n",
    "\n",
    "ax.set_title('Mutual Information: Original vs Smoothed', fontsize=16)\n",
    "ax.set_xlabel('Mutual Information Score', fontsize=12)\n",
    "ax.set_ylabel('')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "10032d9769443475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "def plot_features(df1, df2, feature_list):\n",
    "    for feature in feature_list:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(24, 8))  # 3 columns, 1 row\n",
    "\n",
    "        # Extract unique labels\n",
    "        classes = sorted(df1[\"label\"].unique())  # Assuming labels are the same in both datasets\n",
    "\n",
    "        # Prepare boxplot data for each class\n",
    "        data1_box = [df1[df1[\"label\"] == lbl][feature].dropna() for lbl in classes]\n",
    "        data2_box = [df2[df2[\"label\"] == lbl][feature].dropna() for lbl in classes]\n",
    "\n",
    "        # Boxplot for both datasets (stacked & colored)\n",
    "        box1 = axes[0].boxplot(data1_box, positions=np.arange(len(classes)) - 0.2, widths=0.3, patch_artist=True,\n",
    "                               boxprops=dict(facecolor=\"lightblue\"), medianprops=dict(color=\"black\"))\n",
    "        box2 = axes[0].boxplot(data2_box, positions=np.arange(len(classes)) + 0.2, widths=0.3, patch_artist=True,\n",
    "                               boxprops=dict(facecolor=\"lightcoral\"), medianprops=dict(color=\"black\"))\n",
    "\n",
    "        axes[0].set_xticks(range(len(classes)))\n",
    "        axes[0].set_xticklabels(classes)\n",
    "        axes[0].set_title(f\"Boxplot of {feature} by Label (Data1 vs Data2)\")\n",
    "        axes[0].set_xlabel(\"Label\")\n",
    "        axes[0].set_ylabel(feature)\n",
    "        axes[0].legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], [\"Data1\", \"Data2\"], loc=\"upper right\")\n",
    "\n",
    "        # Scatter plot limits\n",
    "        vmin, vmax = np.percentile(df1[feature].dropna(), [0.2, 99.8])\n",
    "        vmin2, vmax2 = np.percentile(df2[feature].dropna(), [0.2, 99.8])\n",
    "        vmin = min(vmin, vmin2)\n",
    "        vmax = max(vmax, vmax2)\n",
    "\n",
    "        # Scatter plot for df1 (Data1)\n",
    "        df1_label_0 = df1[df1[\"label\"] == 0]\n",
    "        df1_other_labels = df1[df1[\"label\"] != 0]\n",
    "\n",
    "        axes[1].scatter(df1_label_0[\"posx\"], df1_label_0[\"posy\"], c=np.clip(df1_label_0[feature], vmin, vmax),\n",
    "                        cmap=\"viridis\", vmin=vmin, vmax=vmax, marker='o', label=\"Label 0\")\n",
    "        axes[1].scatter(df1_other_labels[\"posx\"], df1_other_labels[\"posy\"], c=np.clip(df1_other_labels[feature], vmin, vmax),\n",
    "                        cmap=\"viridis\", vmin=vmin, vmax=vmax, marker='x', label=\"Other Labels\")\n",
    "        axes[1].set_xlabel(\"Posx\")\n",
    "        axes[1].set_ylabel(\"Posy\")\n",
    "        axes[1].set_title(f\"2D Plot of {feature} (Data1)\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        # Scatter plot for df2 (Data2)\n",
    "        df2_label_0 = df2[df2[\"label\"] == 0]\n",
    "        df2_other_labels = df2[df2[\"label\"] != 0]\n",
    "\n",
    "        axes[2].scatter(df2_label_0[\"posx\"], df2_label_0[\"posy\"], c=np.clip(df2_label_0[feature], vmin, vmax),\n",
    "                        cmap=\"viridis\", vmin=vmin, vmax=vmax, marker='o', label=\"Label 0\")\n",
    "        axes[2].scatter(df2_other_labels[\"posx\"], df2_other_labels[\"posy\"], c=np.clip(df2_other_labels[feature], vmin, vmax),\n",
    "                        cmap=\"viridis\", vmin=vmin, vmax=vmax, marker='x', label=\"Other Labels\")\n",
    "        axes[2].set_xlabel(\"Posx\")\n",
    "        axes[2].set_ylabel(\"Posy\")\n",
    "        axes[2].set_title(f\"2D Plot of {feature} (Data2)\")\n",
    "        axes[2].legend()\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Call function with datasets\n",
    "plot_features(data, smoothed_df, feature_list)\n"
   ],
   "id": "e27c96d5c2aee587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# Predictions\n",
    "svm_y_train_pred = svm_model.predict(X_train_adasyn)\n",
    "svm_y_test_pred = svm_model.predict(X_test_selected)\n",
    "knn_y_train_pred = knn_model.predict(X_train_adasyn)\n",
    "knn_y_test_pred = knn_model.predict(X_test_selected)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_train, y_train_pred, y_test, y_test_pred, model_name):\n",
    "    print(f\"\\n{model_name} Classification Report (Test Data):\")\n",
    "    print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_bal_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    train_bal_acc = balanced_accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    print(f\"{model_name} Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"{model_name} Training Balanced Accuracy: {train_bal_acc:.4f}\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"{model_name} Test Balanced Accuracy: {test_bal_acc:.4f}\")\n",
    "\n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_y_train_pred, svm_y_test_pred = evaluate_model(y_train_adasyn, svm_y_train_pred, y_test, svm_y_test_pred, \"SVM\")\n",
    "\n",
    "# Evaluate KNN\n",
    "knn_y_train_pred, knn_y_test_pred = evaluate_model(y_train_adasyn, knn_y_train_pred, y_test, knn_y_test_pred, \"KNN\")\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "plot_confusion_matrix(y_train_adasyn, svm_y_train_pred, 'SVM Training Confusion Matrix', axes[0, 0])\n",
    "plot_confusion_matrix(y_test, svm_y_test_pred, 'SVM Test Confusion Matrix', axes[0, 1])\n",
    "plot_confusion_matrix(y_train_adasyn, knn_y_train_pred, 'KNN Training Confusion Matrix', axes[1, 0])\n",
    "plot_confusion_matrix(y_test, knn_y_test_pred, 'KNN Test Confusion Matrix', axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4bbde64cffef6a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features Extraction",
   "id": "3c644c2cbbab7841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = {idx: pd.DataFrame({'time': row['t'], 'Fz_s': row['Fz_s']})\n",
    "       for idx, row in data.iterrows()}\n"
   ],
   "id": "6a9899dbfcee2d28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "long_df = pd.concat([df.assign(id=idx) for idx, df in dfs.items()], ignore_index=True)",
   "id": "28e889155824ae70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Step 1: Extract features from the time series data.\n",
    "extracted_features = extract_features(long_df, column_id=\"id\", column_sort=\"time\", column_value=\"Fz_s\")\n",
    "\n",
    "# Step 2: Impute any missing values that might have resulted from the extraction process.\n",
    "imputed_features = impute(extracted_features)\n",
    "\n",
    "# Step 3: Select features that significantly correlate with the labels.\n",
    "relevant_features = select_features(imputed_features, data[\"label\"])"
   ],
   "id": "986be02ef9cb2aaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "relevant_features = select_features(imputed_features, data[\"label\"], fdr_level=0.0000001)  # Stricter selection\n",
    "print(relevant_features.shape)\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Compute mutual information scores\n",
    "mi_scores = mutual_info_classif(imputed_features, data[\"label\"])\n",
    "\n",
    "# Create DataFrame with feature importance\n",
    "feature_importance = pd.DataFrame({\"feature\": imputed_features.columns, \"importance\": mi_scores})\n",
    "\n",
    "# Sort features by importance (highest first)\n",
    "top_20_features = feature_importance.sort_values(by=\"importance\", ascending=False).head(20)\n",
    "\n",
    "# Extract the corresponding feature columns from the dataset\n",
    "top_20_feature_names = top_20_features[\"feature\"].tolist()\n",
    "top_20_selected = imputed_features[top_20_feature_names]\n",
    "\n",
    "print(f\"Top 20 Most Informative Features:\\n{top_20_selected.columns}\")\n"
   ],
   "id": "b64850d351429e34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(top_20_selected, data[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ],
   "id": "3d35fa6f8636f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=set(data[\"label\"]), yticklabels=set(data[\"label\"]))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "3dd888166c0ee379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure feature_list contains only valid feature names\n",
    "feature_list = top_20_selected.columns  # Adjust if necessary\n",
    "\n",
    "for feature in feature_list:\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8, 8))  # Two side-by-side plots\n",
    "\n",
    "    # Scatter plot: Feature values in 2D space\n",
    "    vmin, vmax = np.percentile(top_20_selected[feature], [1, 99])  # Clip extreme values\n",
    "    sc = axes.scatter(data[\"posx\"], data[\"posy\"], c=np.clip(top_20_selected[feature], vmin, vmax),\n",
    "                      cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Add color bar\n",
    "    cbar = plt.colorbar(sc, ax=axes)\n",
    "    cbar.set_label(feature)\n",
    "\n",
    "    axes.set_xlabel(\"Posx\")\n",
    "    axes.set_ylabel(\"Posy\")\n",
    "    axes.set_title(f\"{feature} Distribution in 2D Space\")\n",
    "\n",
    "    # Adjust layout and remove default Matplotlib boxplot title\n",
    "    plt.suptitle(\"\")  # Remove default title generated by `boxplot`\n",
    "    plt.tight_layout()  # Prevent overlap\n",
    "    plt.show()\n"
   ],
   "id": "8ae9b133ee3dd50d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
